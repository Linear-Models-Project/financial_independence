---
title: "Logistic Regression - Home Ownership"
author: "Isaac Levy"
date: "2024-08-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(openintro)
library(nationalparkcolors)
library(PrettyCols)
library(MASS)
library(broom)
library(caret)
library(car)
library(glmnet)
library(pls)
library(corrr)
library(factoextra)
library(FactoMineR)
library(tidyverse)


acadia <- park_palette('Acadia')
autumn <- prettycols('Autumn')
```

## Data Import and Cleaning

```{r data setup}
data('reddit_finance')

# select only respondants from the US
df <- reddit_finance[reddit_finance$country == 'United States',]

# remove all columns with more than 50% missing values
df.2 <- df[, which(colMeans(!is.na(df)) > 0.5)]

# remove all NAs in housing column
df.2 <- df.2[!is.na(df.2$housing),]

# recode housing variable to binary
df.2 <- df.2 %>%
  mutate(numown = ifelse(housing == 'Own', 1, 0))

# drop columns we know we don't care about (i.e., country is all US, other columns have many NAs, etc.)
df.6 <- df.2 %>%
  select(-c(other_val, spec_crypto, invst_prop_bus_own, '2020_invst_save', 
            '2020_healthcare_exp', country, housing))

# drop any all NAs from current data set - left with 609 obs and 42 vars
df.nona <- na.omit(df.6)

# after some lasso variable selection and stepAIC models, select some variables of interest
df.new <- select(df.nona, c(numown, fin_indy_pct, home_value,
                            retirement_accts_tax, '2020_housing_exp',
                            '2020_gross_inc', '2020_utilities_exp',
                            '2020_necessities_exp', age, rel_status, children,))

# rename some variables to not start with numbers
df.new <- rename(df.new, c('utilities_exp' = '2020_utilities_exp',
                           'necessities_exp' = '2020_necessities_exp',
                           'gross_inc' = '2020_gross_inc',
                           'housing_exp' = '2020_housing_exp'))

# drop columns with insignificant p-values in model
df.new <- df.new %>% select(-c(necessities_exp, fin_indy_pct, home_value))

# it turns out these columns have text 'N/A's, drop them
df.new <- df.new %>%
  filter(children != 'N/A',
         rel_status != 'N/A',
         age != 'N/A')

# drop more columns that have no significance, let with 596 obs and 4 3 predictors
final <- df.new %>% select(-c(retirement_accts_tax, age, rel_status, gross_inc))
```

## The Model

```{r}
mod1 <- glm(numown ~ ., data = final, family = binomial)
summary(mod1)
```

### Residuals

```{r}
quantile(residuals(mod1))
```

### VIF

```{r}
vif(mod1)
```

### Prediction Evaluation

```{r}
# predictions
y_hat <- round(predict(mod1, type = 'response'), 3)
# rounded predictions
y_hat_rounded <- ifelse(y_hat > .50, 1, 0)
# actual values
y <- final$numown
# residuals
r <- round(residuals(mod1, type = 'response'), 3)
# bind columns together
model.eval <- data.frame(cbind(y, y_hat_rounded, y_hat, r))
head(model.eval)
```

## PCA

```{r}
# select only numeric columns and numown
df.numeric <- df.2 %>%
  select(c(numown, '2020_gross_inc', '2020_housing_exp', '2020_utilities_exp', 
           '2020_transp_exp', '2020_necessities_exp', '2020_lux_exp',
           '2020_invst_save', '2020_healthcare_exp', '2020_taxes', cash,
           retirement_accts_tax,brokerage_accts_tax, home_value, retire_exp,
           retire_invst_num, fin_indy_num, fin_indy_pct)) %>%
  na.omit()

df.pca <- princomp(df.numeric[-1], fix_sign = T, cor = T)

pca.dat <- data.frame(numown=df.numeric$numown, df.pca$scores)

logit.pca <- glm(numown~., data = pca.dat, family = binomial)
logit.pca.aic <- stepAIC(logit.pca, direction = 'both', trace = F, method = 'glm')

summary(logit.pca.aic)
```

```{r}
# predictions
y_hat.pca <- round(predict(logit.pca.aic, type = 'response'), 3)
# rounded predictions
y_hat_rounded.pca <- ifelse(y_hat.pca > .50, 1, 0)
# actual values
y.pca <- pca.dat$numown
# residuals
r.pca <- round(residuals(logit.pca.aic, type = 'response'), 3)
# bind columns together
model.eval.pca <- data.frame(cbind(y.pca, y_hat_rounded.pca, y_hat.pca, r.pca))
head(model.eval.pca)
```

```{r}
ctrl <- trainControl(method = 'cv', number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = T, savePredictions = T)

pca.dat$numown <- ifelse(pca.dat$numown == 1, 'DoesOwn', 'NotOwn')

logit.pca.train <- train(numown~., method = 'glm', data = pca.dat, family = binomial,
                       trControl = ctrl, metric = 'ROC')
# i missed some steps here
predictions <- logit.pca.train$pred

roc <- roc(predictions$obs, predictions$DoesOwn)

roc.dat <- data.frame(TPR = roc$sensitivities, FPR = 1-roc$specificities)

ggplot(roc.dat, aes(x=FPR, y=TPR))+geom_line(color='forestgreen')
```

```{r}
roc
```


```{r}
df.numeric <- df.numeric %>%
  rename(c(healthcare_exp = '2020_healthcare_exp',
           housing_exp = '2020_housing_exp',
           gross_inc = '2020_gross_inc',
           utilities_exp = '2020_utilities_exp',
           transp_exp = '2020_transp_exp',
           lux_exp = '2020_lux_exp',
           invst_save = '2020_invst_save', 
           taxes = '2020_taxes'))

df.numeric.trans <- df.numeric %>%
  mutate(retire_invst_num = log(1+retire_invst_num),
         healthcare_exp = log(1+healthcare_exp),
         housing_exp = log(1+housing_exp),
         fin_indy_num = log(1+fin_indy_num),
         invst_save = log(1+invst_save))


model.df <- gather(df.numeric.trans, key = 'predictor', value = 'value', -numown)

model.df %>%
  ggplot() +
  aes(y=numown, x=value) +
  geom_jitter() +
  facet_wrap(predictor~., scale = 'free') +
  geom_smooth(method = 'lm')

```


#### Confusion Matrix for PCA

```{r}
expected.vals.pca <- factor(df.numeric$numown)
predicted.vals.pca <- factor(ifelse(predict(logit.pca.aic) > .7, 1, 0))
confuse.pca <- confusionMatrix(data = predicted.vals.pca,
                               reference = expected.vals.pca)
confuse.pca
```

### Confusion Matrix for Regular GLM

```{r}
expected.vals <- factor(final$numown)
predicted.vals <- factor(ifelse(predict(mod1) > .7, 1, 0))
confuse <- confusionMatrix(data = predicted.vals, reference = expected.vals)
confuse
```

### Confusion Matrix for FAMD

```{r}
expected.vals.famd <- factor(df.6$numown)
predicted.vals.famd <- factor(ifelse(predict(mca.mod.aic) > .7, 1, 0))
confuse.famd <- confusionMatrix(data = predicted.vals.famd,
                                reference = expected.vals.famd)
confuse.famd
```



```{r}
quantile(residuals(logit.pca.aic))
```


```{r}
# PCA residuals look really good

model.eval.pca %>%
  ggplot() +
  aes(x=r.pca) +
  geom_density(stat = 'count')
```

```{r}
# We can account for all of the variance with these principal components!!
summary(df.pca)
```


```{r}
# Non-PCA residuals look worse than PCA residuals

model.eval %>%
  ggplot() +
  aes(x=r) +
  geom_histogram(stat = 'count')
```

## Principal components of mixed types using FactoMineR
```{r}
data('reddit_finance')

# select only respondants from the US
df <- reddit_finance[reddit_finance$country == 'United States',]

# remove all columns with more than 50% missing values
df.2 <- df[, which(colMeans(!is.na(df)) > 0.5)]

# remove all NAs in housing column
df.2 <- df.2[!is.na(df.2$housing),]

# recode housing variable to binary
df.2 <- df.2 %>%
  mutate(numown = ifelse(housing == 'Own', 1, 0))

# drop columns we know we don't care about (i.e., country is all US, other columns have many NAs, etc.)
df.6 <- df.2 %>%
  select(-c(other_val, spec_crypto, invst_prop_bus_own, '2020_invst_save', 
            '2020_healthcare_exp', country, housing)) %>%
  na.omit()

res.famd <- FAMD(df.6,
                 ncp = 41,
                 sup.var = 42,
                 graph = F)

mca.mod.data <- data.frame(numown=df.6$numown, res.famd$svd$U)

mca.mod.max <- glm(numown~., data = mca.mod.data, family = binomial)

mca.mod.aic <- stepAIC(mca.mod.max, direction = 'both', trace = F, method = 'glm')
summary(mca.mod.aic)
```

```{r}
# predictions
y_hat.famd <- round(predict(mca.mod.aic, type = 'response'), 3)
# rounded predictions
y_hat_rounded.famd <- ifelse(y_hat.famd > .50, 1, 0)
# actual values
y.famd <- mca.mod.data$numown
# residuals
r.famd <- round(residuals(mca.mod.aic, type = 'response'), 3)
# bind columns together
model.eval.famd <- data.frame(cbind(y.famd, y_hat_rounded.famd,
                                    y_hat.famd, r.famd))
head(model.eval.famd)
```


```{r}
# residuals look better than base model but maybe worse than PCA model

model.eval.famd %>%
  ggplot() +
  aes(x=r.famd) +
  geom_histogram(stat = 'count')
```

```{r}
# All of the dimensions only account for 53.74% of variance!!! Boo
get_eigenvalue(res.famd)
```

