---
title: "Cleaned Logistic Regression"
author: "Isaac Levy"
date: "2024-08-07"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup}
suppressPackageStartupMessages({
library(openintro)
library(nationalparkcolors)
library(PrettyCols)
library(MASS)
library(broom)
library(caret)
library(car)
library(glmnet)
library(pls)
library(corrr)
library(pROC)
library(tidyverse)


acadia <- park_palette('Acadia')
autumn <- prettycols('Autumn')
})
```

## Data Import and Cleaning

```{r data import and cleaning}
data('reddit_finance') # from the openintro library

# select only respondants from the US
df <- reddit_finance[reddit_finance$country == 'United States',]

# remove all columns with more than 50% missing values
df.2 <- df[, which(colMeans(!is.na(df)) > 0.5)]

# remove all NAs in housing column
df.2 <- df.2[!is.na(df.2$housing),]

# recode housing variable to binary
df.2 <- df.2 %>%
  mutate(numown = ifelse(housing == 'Own', 1, 0))

# select only numeric columns and numown
df.numeric <- df.2 %>%
  select(c(numown, '2020_gross_inc', '2020_housing_exp', '2020_utilities_exp', 
           '2020_transp_exp', '2020_necessities_exp', '2020_lux_exp',
           '2020_invst_save', '2020_healthcare_exp', '2020_taxes', cash,
           retirement_accts_tax,brokerage_accts_tax, home_value, retire_exp,
           retire_invst_num, fin_indy_num, fin_indy_pct)) %>%
  na.omit()

# rename variables that start with a number
df.numeric <- df.numeric %>%
  rename(c(healthcare_exp = '2020_healthcare_exp',
           housing_exp = '2020_housing_exp',
           gross_inc = '2020_gross_inc',
           utilities_exp = '2020_utilities_exp',
           transp_exp = '2020_transp_exp',
           lux_exp = '2020_lux_exp',
           invst_save = '2020_invst_save', 
           taxes = '2020_taxes'))
# transform variables with skew by using lot(1+variable) since many had 0s
df.numeric.trans <- df.numeric %>%
  mutate(retire_invst_num = log(1+retire_invst_num),
         healthcare_exp = log(1+healthcare_exp),
         housing_exp = log(1+housing_exp),
         fin_indy_num = log(1+fin_indy_num),
         invst_save = log(1+invst_save))
```



## The Model

```{r build model}
suppressWarnings({
# create princomp of remaining numeric variables
df.pca <- princomp(df.numeric.trans[-1], fix_sign = T, cor = T)
# create dataframe with princomp scores and response variable
pca.dat <- data.frame(numown=df.numeric.trans$numown, df.pca$scores)
# build logistic model will ~all~ components
logit.pca <- glm(numown~., data = pca.dat, family = binomial)
# use stepAIC to trim model
logit.final <- stepAIC(logit.pca, direction = 'both', trace = F, method = 'glm')
# summary of best model as determined by stepAIC
summary(logit.final)
})
```

## Model Predictions vs Observed

Checking true values vs predicted, using a threshold of .7. Calculated residuals, $\hat{y}$ , and rounded $\hat{y}$  for each observation. Displaying rows 50-100 as an example.

```{r make predictions for each observation}
# predictions
y_hat<- round(predict(logit.final, type = 'response'), 3)
# rounded predictions
y_hat_rounded <- ifelse(y_hat > .70, 1, 0)
# actual values
y <- pca.dat$numown
# residuals
resids <- round(residuals(logit.final, type = 'response'), 3)
# bind columns together
model.eval<- data.frame(cbind(y, y_hat_rounded, y_hat, resids))
model.eval[50:100,]
```

### Residuals

Residuals look really good

#### Residual Distribution Plot

```{r plot residual dist}
model.eval %>%
  ggplot() +
  aes(x=resids) +
  geom_density(stat = 'count')
```

#### Residuals Quantiles

Centered near 0, 2nd and 3rd quantiles have similar magnitude, some negative skew for lower values.

```{r residual quantiles}
quantile(residuals(logit.final))
```


### Confusion Matrix

Using a threshold of .7, we still have 94.33% accuracy. 95% confidence interval of (0.9208, 0.9609).

```{r confusion matrix}
expected.vals.pca <- factor(df.numeric.trans$numown)
predicted.vals.pca <- factor(ifelse(predict(logit.final) > .7, 1, 0))
confuse.pca <- confusionMatrix(data = predicted.vals.pca,
                               reference = expected.vals.pca)
confuse.pca
```

### ROC Curve and Area Under Curve

ROC curve looks very strong.

```{r roc curve}
suppressWarnings({
ctrl <- trainControl(method = 'cv', number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = T, savePredictions = T)

dat <- pca.dat

dat$numown <- ifelse(dat$numown == 1, 'DoesOwn', 'NotOwn')

logit.pca.train <- train(numown~., method = 'glm', data = dat,
                         family = binomial, trControl = ctrl, metric = 'ROC')
})
predictions <- logit.pca.train$pred

roc <- roc(predictions$obs, predictions$DoesOwn)

roc.dat <- data.frame(TPR = roc$sensitivities, FPR = 1-roc$specificities)

ggplot(roc.dat, aes(x=FPR, y=TPR))+geom_line(color=autumn[1])
```

Area under curve is very close to 0.

```{r roc value}
roc
```



## Check Model Assumptions

### Relationship Between Log Odds and Components

Most of the components have a linear relationship with the log odds, but there are some exceptions. Components 4, 6, and 11 are noticeably not great, but the rest look better.

```{r log odds}
# create vector of log-odds
probabilities <- predict(logit.final, type = 'response')
logit = log(probabilities/(1-probabilities))
# combine log-odds with pca components
pca.dat.assumptions <- cbind(pca.dat, logit)
# melt dataframe to be able to facet wrap
model.df <- gather(pca.dat.assumptions, key = 'predictor', value = 'value', -c(logit, numown))
# graph log-odds vs principal components
model.df %>%
  ggplot() +
  aes(x=value, y=logit) +
  geom_jitter() +
  facet_wrap(predictor~., scale = 'free') +
  geom_smooth(method = 'glm', color=autumn[1])
```

### Multicollinearity

No multicollinearity, although this is expected because of PCA.

```{r multicollinearity}
vif(logit.final)
```


